Web Audio API
=============

Web Audio API - это технология, позволяющая существенно расширить возможности воспроизведения звука в браузере. К сожалению эта технология на данный момент очень молодая и её поддержка есть только в свежих версиях популярных десктопных браузеров и практически отсутствует в мобильных браузерах.
  
У Web Audio API стоит выделить 3 основных аспекта, которые особо не пересекаются и их можно рассматривать отдельно:
 
  - возможности работы с различными источниками сигнала
  - возможности по анализу сигнала
  - возможности по цифровой обработке сигнала
  
Источники сигнала
-----------------

Web Audio API работает с цифровыми сигналами, которые могут быть получены из самых разных источников или созданы непосредственно с помощью скриптов. Элементы Web Audio API, которые могут генерировать некий сигнал будем называть источниками сигнала, а любые посторониие объекты, которые могут быть использованы для генерации этого сигнала будем называть источниками данных.

В отличие от медиа-элементов у Web Audio API нет методов получения данных по сети или из файловой системы. Вместо этого есть 3 типа источников данных, которые позволяют решать проблему получения данных самыми разнообразными способами, которые не доступны медиа-элементам.

### [MediaElementAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaElementAudioSourceNode)

Начнём с самого простого. Источником данных этого элемента является ```<audio>``` или ```<video>``` элемент. 

Фабрика ```AudioContext#createMediaElementSource``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaElementSource)) принимает в качестве единственного параметра медиа-элемент (```<audio>``` или ```<video>```). При этом создание ```MediaElementAudioSourceNode``` перенапрявляет вывод аудио-потока из медиа-элемента в данный объект. Остальное поведение медиа-элемента при этом не изменяется. 

Стоит отметить 2 вещи:
  
   - у медиа-элемента по прежнему работают функции изменения громкости. Для целей воспроизведения это может не иметь особого значения, но если требуется проводить анализ сигнала лучше всего выставить медиа-элементу максимальную громкость, а итоговую громкость на выходе регулировать с помощью GainNode (об этом ещё будет рассказано позднее)
   - Web Audio API требует специальных прав на получение данных из медиа-элемента. Если трек загружается с другого сервера, то для корректной работы ```MediaElementAudioSourceNode``` требуется, чтобы у медиа-элемента был выставлен параметр ```crossOrigin``` и в ответах сервера приходил корректный заголовок ```Access-Control-Allow-Access```. В противном случае этот элемент нельзя будет использовать в качестве источника данных для Web Audio API.

### [MediaStreamAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode)

Источником данных этого элемента является аудио-вход или любой другой источник настроенный в системе или браузере пользователя. 

Фабрика ```AudioContext#createMediaStreamSource``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaStreamSource)) принимает в качестве единственного параметра медиа-поток, получаемый из метода ```MediaDevices.getUserMedia``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)). Основное применение данного источника - использование микрофона или линейного входа в качестве источника аудио-данных. Тут вроде подводных камней нет.

### [AudioBufferSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode)

Источником данных этого элемента является [AudioBuffer](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer). Это пожалуй самый интересный тип источников сигнала, т.к. он предоставляет те возможности, которые отсутсвуют у медиа-элементов. С помощью аудио-буфера можно формировать любой сигнал непосредственно в браузере (например таким образом это делается здесь [wavepot.com](http://wavepot.com/)). 

Фабрика ```AudioContext#createBufferSource``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBufferSource)) в отличие от предыдущих источников не принимает элемент буфера в качестве аргумента - он устанавливается вручную после создания элемента (например ```audioBufferSource.buffer = audioBuffer```).

AudioBuffer может быть сформирован множеством разных способов. 

#### Непосредственное создание и заполнение буфера

Самый банальный метод - непосредственное создание и заполнение этого буфера. Конструктор ```AudioContext#createBuffer``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBuffer)) принимает 3 параметра:

  - количество каналов
  - количество семплов
  - частоту дискретизации

Буфер может иметь до 32 каналов. Каждый канал содержит информацию о сигнале в формате PCM.

#### Декодирование сжатых аудио-данных

Также можно создать буфер из сжатых аудио-данных. Для этого используется метод ```AudioContext#decodeAudioData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/decodeAudioData)). Он принимает 2 параметра:

  - сжатые аудио-данные
  - обработчик в который будет передан объект ```AudioBuffer``` с декодированными данными
  
Благодаря этому методу можно не только вручную контролировать процесс загрузки данных, но и допустим сохранять их в localStorage (для больших файлов не подойдёт, но короткий "блямк" вполне можно сохранить). Однако стоит помнить про то, что буфер содержит несжатые данные. Каждый семпл в каждом канале - это 32 битное число с плавающей точкой, так что 5 минутный стерео-трек займёт 106 мегабайт памяти. Так что если вы решите загрузить целиком 9 симфонию Бетховена (длительность аудио-диска 74 минуты - длительность этой симфонии), то она займёт 783 мегабайта в моно-режиме и 1,5 гигабайта в стерео.

#### Использование [OfflineAudioContext](https://developer.mozilla.org/en-US/docs/Web/API/OfflineAudioContext)

Самый нетривиальный метод создания аудио-буффера. Элемент ```OfflineAudioContext``` работает примерно также как обычный ```AudioContext```, но вместо вывода аудио-потока на устройства воспроизведения, он рассчитывает аудио-буфер. Конструктор принимает те же параметры что и ```AudioContext#createBuffer```. 

Фактически данный элемент позволяет произвести необходимые преобразования звука заранее, а затем многократно переиспользовать полученный аудио-буфер вместо повторных вычислений в рантайме. Возможности по формированию и обработке сигнала в данном случае ограничены только воображением.

### [OscillatorNode](https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode)

Этот элемент - самый простой и логичный генератор сигналов. Он просто генерирует периодический сигнал с заданной частотой. Форму волны при этом можно задавать указывая один из заранее заданных [типов](https://developer.mozilla.org/en-US/docs/Web/API/OscillatorNode/type) или с помощью элементов ```PeriodicWave``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/PeriodicWave)).

Фабрика ```AudioContext#createOscillator``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createOscillator)) не принимает аргументов. Вся настройка происходит уже после создания. (Например ```oscillator.frequency.value = 3000```).

```PeriodicWave``` - является элементом, описывающим форму некоторой периодически повторяющейся волны. Форма волны задаётся с помощью коэффициентов для обратного Быстрого Преобразование Фурье. Максимальное колличество коэфициентов = 4096

Фабрика ```AudioContext#createPeriodicWave``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createPeriodicWave)) принимает 3 аргумента:

  - массив коэффициентов для обратного Быстрого Преобразования Фурье (действительная часть)
  - массив коэффициентов для обратного Быстрого Преобразования Фурье (мнимая часть)
  - опциональный объект ```{disableNormalization: true}```, отключающий нормализацию волны

Чтобы понять как разные коэфициенты влияют на форму волны можно посмотреть данную [демонстрацию](https://www.desmos.com/calculator/lmzpimpbou). Под графиком расположены 8 контрольных точек, которые можно передвигать чтобы изменять коэффициенты. По оси x отсчитывается действительная часть коэффициента, по оси y - мнимая. Графики отображают действительную и мнимую часть волны после обратного Преобразования Фурье.

Стоит учитывать однако, что вне зависимости от количеста коэффициентов, которые будут переданы в фабрику общее количество отсчётов, которое будет использовано для обратного преобразования будет равно 4096. **ТРЕБУЕТСЯ ПЕРЕПРОВЕРИТЬ ПРАВИЛЬНО ЛИ Я РАЗОБРАЛСЯ В КОДЕ**

Анализ сигнала
--------------

Для анализа данных аудио-потока можно использовать 2 различных подхода:

  - анализ данных непосредственно из аудио-буффера (Web Audio API в этом нам не помощник, оно только предоставит массив данных - дальше нужно самостоятельно описывать функции обработки этих данных)
  - анализ данных с помощью ```AnalyserNode``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode))
  
### AudioBuffer

Я не буду здесь расписывать каким образом можно использовать данные аудио-буфера, т.к. это не относится к Web Audio API напрямую. Получить данные любого канала можно с помощью методов ```AudioBuffer#getChannelData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer/getChannelData)) и ```AudioBuffer#copyFromChannel``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer/copyFromChannel))

### [AnalyzerNode](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode)

Данный элемент используется для анализа данных аудио-потока в реальном времени. 

Фабрика ```AudioContext#createAnalyser``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createAnalyser)) не принимает никаких аргументов. Все настройки осуществляются после создания элемента. (Например ```analyser.fftSize = 2048;```)

Фактически данный элемент содержит в себе буффер данных, который заполняется по мере воспроизведения потока. К этому буферу применяется Быстрое Преобразование Фурье. Данные преобразования можно получить с помощью методов ```AnalyserNode#getFloatFrequencyData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatFrequencyData), передаёт данные в Float32Array) и ```AnalyserNode#getByteFrequencyData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteFrequencyData), передаёт данные в Uint8Array). Данные анализируемого буфера можно получить с помощью методов ```AnalyserNode#getFloatTimeDomainData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getFloatTimeDomainData), передаёт данные в Float32Array) и ```AnalyserNode#getByteTimeDomainData``` ([man](https://developer.mozilla.org/en-US/docs/Web/API/AnalyserNode/getByteTimeDomainData), передаёт данные в Uint8Array).

#### Быстрое Преобразование Фурье

В статье про [теорию звука](https://github.yandex-team.ru/pages/music/audio/tutorial-sound.html) я постарался избежать описания данного алгоритма, но здесь стоит о нём рассказать, чтобы было понятно для чего он нужен и какие данные с помощью него можно получить.

Быстрое Преобразование Фурье (БПФ), грубо говоря, это ускоренная версия Дискретного Преобразования Фурье (ДПФ), которое позволяет для дискретного сигнала (а цифровой сигнал как раз таким и является) получить набор частотных и фазовых характеристик этого сигнала в виде набора комплексных чисел. Вообще говоря это преобразование над комплексными числами, но в цифровом сигнале у нас есть только вещественная часть, и все мнимые компоненты у нас будут равны нулю. Отличие алгоритмов БПФ И ДПФ пожалуй лишь с в скорости работы и том, что БПФ требует чтобы количество входных данных было равно 2^n. **ПРОВЕРИТЬ ЧТО ЭТО ТАК ДЛЯ БПФ И НЕ ТАК ДЛЯ ДПФ**

Чтобы понять для чего нужно ДПФ проще всего рассмотреть обратное ДПФ. Входными данными для этого преобразования является набор комплексных чисел, которые отражают частотные и фазовые характеристики некого сигнала, а выходными данными является непосредственно сам цифровой сигнал. Каков смысл этих чисел? На самом деле всё просто. Рассмотрим [демонстрацию](https://www.desmos.com/calculator/lmzpimpbou), которую я уже приводил в разделе про ```PeriodicWave```. В этой демонстрации строится непрерывная бесконечная функция, а не дискретный конечный сигнал, но в плане наглядности так даже лучше. Нас будет интересовать только часть ограниченная 2 вертиальными оранжевыми линиями.

Вещественная часть самого первого аргумента всего лишь поднимает/опускает вещественную часть в результирующей функции. Мнимая часть делает тоже самое с мнимой частью функции.
 
Остальные аргументы более интересны. Модуль любого из этих чисел указывает амплитуду некой синусоиды, а аргумент числа - фазу этой синусоиды в точке 0. Длина волны каждой синусоиды зависит от порядкового номера числа L = N / (n - 1), где L - длина волны в системе координат преобразования, N - количество отсчётов, n - порядковый номер аргумента. Таким образом второй аргумент задаёт синусоиду с длинной волны равной количеству отсчётов, третий - с длинной волны в 2 раза меньше, четвёртый - с длинной волны в 3 раза меньше и т.д. Все полученные синусоиды просто складываются друг с другом по принципу суперпозиции.

Собственно если обратное ДПФ из коэффициентов строит цифровой сигнал, то прямое ДПФ из цифрового сигнала получает эти самые коэффициенты, т.е. позволяет разложить наш сигнал в набор косинусоид различной частоты.

#### Интерпретация данных AnalyzerNode

```AnalyzerNode``` возвращает 2 набора данных - частотные характеристики и форму волны. Тут есть несколько подводных камней и особенностей. 

Почему-то нигде в документации не описано в каком формате методы ```AnalyserNode#getFloatFrequencyData``` и ```AnalyserNode#getByteFrequencyData``` возвращают данные, сказано лишь, что эти данные получаются из БПФ. Нам известно, что БПФ возвращает набор комплексных чисел. Первой мыслью было, что данные методы возвращают просто модули этих чисел, но это оказалось не так. Возвращаемые данные условно разделены на 2 равных части. Первая половина данных содержит модули (т.е. значения амплитуды), а вторая часть - аргументы (т.е. значения фазовых сдвигов). И это ещё не всё. Для метода ```AnalyserNode#getFloatFrequencyData``` данные об амплитуде указываются по шкале dBFS, а для метода ```AnalyserNode#getByteFrequencyData``` эта шкала ещё и перенормируется с учётом параметров ```AnalyserNode#minDecibels``` и ```AnalyserNode.maxDecibels```. Не очевидно ещё вот что: размер данных о частотных характеристиках в 2 раза меньше, чем размер буфера (```AnalyserNode.fftSize```), а БПФ должно выдавать данные соответствующие размеру буфера. Половина данных из этих результатов просто выбрасывается. Чуть позже объясню почему для анализа она действительно мало интересна.

Выше была рассмотрена интерпретация значений частотных характеристик в системе координат преобразования, теперь стоит рассмотреть интерпретацию в системе координат цифрового сигнала. Чему равна реальная частота каждого из аргументов? Это зависит от 2х параметров: частоты дискретизации самого цифрового сигнала и размера буфера преобразования. Зависимость частоты от номера аргумента получается такая: D * (n - 1) / B, где D - частота дискретизации сигнала, B - размер буфера, n - номер аргумента. Из этого следует интересное свойство - середина буфера соответствует частоте Найквиста. Именно поэтому половина буфера выбрасывается из выдачи - она описывает частоты выше частоты Найквиста. Частоту дискретизации сигнала можно узнать из ```AudioContext#sampleRate```.

С методами ```AnalyserNode#getFloatTimeDomainData``` и ```AnalyserNode#getByteTimeDomainData``` всё несколько проще - оба метода просто возвращают значения временного буфера. Значения первого идут в диапазоне от -1 до 1, значения второго - от 0 до 255.

Обработка сигнала
----------------

Основное назначние Web Audio API - это обработка сигналов.

Материалы
---------

  * [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
  * [AudioContext](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext)
  * Источники сигнала
    * Медиа-элементы
      * [MediaElementAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaElementAudioSourceNode)
      * [AudioContext#createMediaElementSource](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaElementSource)
      * [CORS](https://developer.mozilla.org/en-US/docs/Web/HTTP/Access_control_CORS)
    * Устройства ввода
      * [MediaStreamAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode)
      * [AudioContext#createMediaStreamSource](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaElementSource)
      * [MediaDevices.getUserMedia](https://developer.mozilla.org/en-US/docs/Web/API/MediaDevices/getUserMedia)
    * Аудио-буфер
      * [AudioBufferSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode)
      * [AudioContext#createBufferSource](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBufferSource)
      * [AudioBuffer](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer)
      * [AudioContext#createBuffer](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBuffer)
      * [wavepot.com](http://wavepot.com/)
      * [AudioContext#decodeAudioData](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/decodeAudioData)
      * [OfflineAudioContext](https://developer.mozilla.org/en-US/docs/Web/API/OfflineAudioContext)
