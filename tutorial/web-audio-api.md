Web Audio API
=============

Web Audio API - это технология, позволяющая существенно расширить возможности воспроизведения звука в браузере. К сожалению эта технология на данный момент очень молодая и её поддержка есть только в свежих версиях популярных десктопных браузеров и практически отсутствует в мобильных браузерах.
  
У Web Audio API стоит выделить 3 основных аспекта, которые особо не пересекаются и их можно рассматривать отдельно:
 
  - возможности работы с различными источниками сигнала
  - возможности по цифровой обработке сигнала
  - возможности по анализу сигнала
  
Источники сигнала
-----------------

Web Audio API работает с цифровыми сигналами, которые могут быть получены из самых разных источников или созданы непосредственно с помощью скриптов. Элементы Web Audio API, которые могут генерировать некий сигнал будем называть источниками сигнала, а любые посторониие объекты, которые могут быть использованы для генерации этого сигнала будем называть источниками данных.

В отличие от медиа-элементов у Web Audio API нет методов получения данных по сети или из файловой системы. Вместо этого есть 3 типа источников данных, которые позволяют решать проблему получения данных самыми разнообразными способами, которые не доступны медиа-элементам.

### [MediaElementAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaElementAudioSourceNode)

Начнём с самого простого. Источником данных этого элемента является ```<audio>``` или ```<video>``` элемент. 

Фабрика ```AudioContext#createMediaElementSource``` ([doc](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaElementSource)) принимает в качестве единственного параметра медиа-элемент (```<audio>``` или ```<video>```). При этом создание ```MediaElementAudioSourceNode``` перенапрявляет вывод аудио-потока из медиа-элемента в данный объект. Остальное поведение медиа-элемента при этом не изменяется. 

Стоит отметить 2 вещи:
  
   - у медиа-элемента по прежнему работают функции изменения громкости. Для целей воспроизведения это может не иметь особого значения, но если требуется проводить анализ сигнала лучше всего выставить медиа-элементу максимальную громкость, а итоговую громкость на выходе регулировать с помощью GainNode (об этом ещё будет рассказано позднее)
   - Web Audio API требует специальных прав на получение данных из медиа-элемента. Если трек загружается с другого сервера, то для корректной работы ```MediaElementAudioSourceNode``` требуется, чтобы у медиа-элемента был выставлен параметр ```crossOrigin``` и в ответах сервера приходил корректный заголовок ```Access-Control-Allow-Access```. В противном случае этот элемент будет нельзя использовать в качестве источника данных для Web Audio API.

### [MediaStreamAudioSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/MediaStreamAudioSourceNode)

Источником данных этого элемента является аудио-вход или любой другой источник настроенный в системе или браузере пользователя. 

Фабрика ```AudioContext#createMediaStreamSource``` ([doc](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createMediaStreamSource)) принимает в качестве единственного параметра медиа-поток, получаемый из метода ```navigator#getUserMedia```. Основное применение данного источника - использование микрофона или линейного входа в качестве источника аудио-данных. Тут вроде подводных камней нет.

### [AudioBufferSourceNode](https://developer.mozilla.org/en-US/docs/Web/API/AudioBufferSourceNode)

Источником данных этого элемента является [AudioBuffer](https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer). Это пожалуй самый интересный тип источников сигнала, т.к. он предоставляет те возможности, которые отсутсвуют у медиа-элементов. С помощью аудио-буфера можно формировать любой сигнал непосредственно в браузере (например таким образом это делается здесь [wavepot.com](http://wavepot.com/)). 

Фабрика ```AudioContext#createBufferSource``` ([doc](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBufferSource)) в отличие от предыдущих источников не принимает элемент буфера в качестве аргумента - он устанавливается вручную после создания элемента (например ```audioBufferSource.buffer = audioBuffer```).

AudioBuffer может быть сформирован множеством разных способов. 

Самый банальный метод - непосредственное создание и заполнение этого буфера. Конструктор ```AudioContext#createBuffer``` ([doc](https://developer.mozilla.org/en-US/docs/Web/API/AudioContext/createBuffer)) принимает 3 параметра:

  - количество каналов
  - количество семплов
  - частоту дискретизации

Пример создания буфера с белым шумом в 2х каналах:

```(javascript)
var channels = 2;
var samples = 22050; // 0.5 sec
var sampleRate = 44100;

audioContext.createBuffer(channels, samples, sampleRate);

var i, channel, channelData;
for (channel = 0; channel < channels; channel++) {
    channelData = audioContext.getChannelData(channel);
        
    for (i = 0; i < samples; i++) {
        channelData[i] = Math.random() * 2 - 1;
    }
}
```

Также можно создать буфер из сжатых аудио-данных. Для этого используется метод ```AudioContext#decodeAudioData```. Он принимает 2 параметра:

  - сжатые аудио-данные
  - обработчик в который будет передан объект ```AudioBuffer``` с декодированными данными
  
Благодаря этому методу можно не только вручную контролировать процесс загрузки данных, но и допустим сохранять их в localStorage (для больших файлов не подойдёт, но короткий "блямк" вполне можно сохранить). Однако стоит помнить про то, что буфер содержит несжатые данные. Каждый семпл в каждом канале - это 32 битное число с плавающей точкой, так что 5 минутный стерео-трек займёт 106 мегабайт памяти. Так что если вы решите загрузить целиком 9 симфонию Бетховена (длительность аудио-диска 74 минуты - длительность этой симфонии), то она займёт 783 мегабайта в моно-режиме и 1,5 гигабайта в стерео.
